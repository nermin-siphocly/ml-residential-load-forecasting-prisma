# -*- coding: utf-8 -*-
"""S1. Abstracts similarity score.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yxg95sWvbfV-BbdQHxAryhIebpNMkkZ8
"""

#STEP 1: Install dependencies
!pip install -U sentence-transformers
!pip install -U huggingface_hub

!pip install -U sentence-transformers huggingface_hub

!pip install pandas
import pandas as pd

import pandas as pd
from sentence_transformers import SentenceTransformer, util
import torch

# Load the model
model = SentenceTransformer('BAAI/bge-large-en-v1.5')#'all-MiniLM-L6-v2')

# Path to your Excel file (upload in Colab or use path if local)
file_path = 'Ex_Abs.xlsx'
#file_path = 'Title Screened.xlsx'

# Load Excel file
try:
    df = pd.read_excel(file_path)
except FileNotFoundError:
    print("Error: Excel file not found. Please make sure the path is correct and the file exists.")
    # Exit or handle the error appropriately
    exit()

# Define the complex topic

topic = """
Electric load forecasting in residential  or household settings.
Not concerned with neither industrial nor commercial nor factory nor non-residential contexts.
Don't include neither agricultural nor irrigation contexts.
Not concerned with neither anomaly detection nor thermal load forecasting nor load monitoring nor electric vehicles.
Exclude review or survey papers. Not concerned with neither solar nor wind energy sources.
"""


'''
topic = """
"This study focuses on electric load forecasting in residential or household settings.
It excludes any work related to industrial, commercial, or factory contexts.
Agricultural and irrigation-related studies are also outside the scope.
The focus is not on anomaly detection, thermal load forecasting, load monitoring, or electric vehicles.
Additionally, review and survey papers are excluded, as well as studies involving solar or wind energy sources.
"""
'''

# Encode the topic once
topic_embedding = model.encode(topic, convert_to_tensor=True)

# Encode the abstracts
abstracts = df["Abstract"].tolist()
abstract_embeddings = model.encode(abstracts, convert_to_tensor=True)

# Compute cosine similarity
abs_cosine_scores = util.cos_sim(abstract_embeddings, topic_embedding).squeeze().tolist()

# Add similarity scores to a new column
df["Abstract Similarity Score"] = abs_cosine_scores

# Encode the Titles
titles = df["Title"].tolist()
title_embeddings = model.encode(titles, convert_to_tensor=True)

# Compute cosine similarity
title_cosine_scores = util.cos_sim(title_embeddings, topic_embedding).squeeze().tolist()

# Add similarity scores to a new column
df["Title Similarity Score"] = title_cosine_scores

# Encode the Titles
tags = df["Tags"].tolist()
tag_embeddings = model.encode(tags, convert_to_tensor=True)

# Compute cosine similarity
tag_cosine_scores = util.cos_sim(tag_embeddings, topic_embedding).squeeze().tolist()

# Add similarity scores to a new column
df["Tag Similarity Score"] = tag_cosine_scores

'''
TAK_embeddings_list = []

for t,k,a in zip(titles,tags,abstracts):
  TAK = "Title: " + t + "\n" + "Keywords: " + k + "\n" + "Abstract: " + a
  TAK_embeddings_list.append(model.encode(TAK, convert_to_tensor=True))

# Stack the list of tensors into a single tensor
TAK_embeddings = torch.stack(TAK_embeddings_list)
'''

# Save back to Excel (overwrite or new file)
output_file = "scored_abstracts.xlsx"
df.to_excel(output_file, index=False)

print(f"Similarity scores saved to {output_file}")

