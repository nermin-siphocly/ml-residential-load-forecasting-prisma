# -*- coding: utf-8 -*-
"""Another copy of Abstracts_Classification (DistilBERT hybrid + support list).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uqfvw1x0yke_guikTS1J0Ss_mOtFWdyK
"""

!pip install transformers sentence-transformers pandas torch

import torch
import pandas as pd
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from sentence_transformers import SentenceTransformer, util

# Load cross-encoder classifier
model_name = "distilbert-base-uncased"
model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=15)
tokenizer = DistilBertTokenizer.from_pretrained(model_name)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# Labels and their indices
candidate_labels = [
    "electrical load forecasting in residential areas only",
    "non-residential contexts",
    "commercial buildings",
    "manufacturing or factory contexts",
    "agricultural energy systems",
    "primary focus on anomaly detection",
    "load monitoring as primary focus",
    "thermal load forecasting",
    "heating or cooling systems",
    "electric vehicles",
    "explicitly stating this is a literature review or a survey paper",
    "solar energy",
    "wind energy",
    "renewable energy",
    "unclear or mixed focus"
]
label_map = {label: i for i, label in enumerate(candidate_labels)}
index_to_label = {v: k for k, v in label_map.items()}

# Load your support examples
support_examples = pd.read_excel("support_examples.xlsx")  # columns: text, label
support_texts = support_examples["text"].tolist()
support_labels = support_examples["label"].tolist()

# Embed support examples
#embedder = SentenceTransformer("all-MiniLM-L6-v2")
embedder = SentenceTransformer("facebook/galactica-6.7b")
#facebook/galactica-6.7b
support_embeddings = embedder.encode(support_texts, convert_to_tensor=True)

# Load target abstracts
df = pd.read_excel("to_test.xlsx")
keys = df["Key"].tolist()
abstracts = df["Abstract"].tolist()

# Classify using cross-encoded input
results = []
for key, abstract in zip(keys, abstracts):
    # Step 1: Retrieve most similar support example
    abstract_embedding = embedder.encode(abstract, convert_to_tensor=True)
    cosine_scores = util.cos_sim(abstract_embedding, support_embeddings).squeeze()
    top_idx = torch.argmax(cosine_scores).item()
    similarity = max(0.0, min(cosine_scores[top_idx].item(), 1.0))  # Clamp to [0,1]

    support_text = support_texts[top_idx]
    support_label = support_labels[top_idx]
    label_id = label_map[support_label]

    # Step 2: Tokenize input + support together
    combined_input = tokenizer(
        abstract,
        support_text,
        truncation=True,
        padding="max_length",
        max_length=512,
        return_tensors="pt"
    )
    combined_input = {k: v.to(device) for k, v in combined_input.items()}

    # Step 3: Model prediction
    with torch.no_grad():
        outputs = model(**combined_input)
        logits = outputs.logits.squeeze()

    # Step 4: Create one-hot retrieval logits (scaled by similarity)
    retrieval_logits = torch.zeros_like(logits)
    retrieval_logits[label_id] = similarity

    # Step 5: Blend using dynamic alpha
    alpha = 1.0 - similarity
    final_logits = alpha * logits + (1 - alpha) * retrieval_logits
    probs = torch.softmax(final_logits, dim=-1)
    pred_idx = torch.argmax(probs).item()

    # Step 6: Save result
    results.append({
        "key": key,
        "abstract_text": abstract,
        "predicted_label": index_to_label[pred_idx],
        "retrieved_example": support_text,
        "retrieved_label": support_label,
        "similarity_score": similarity,
        "alpha_used": alpha,
        "confidence": probs[pred_idx].item()
    })

# Save predictions
results_df = pd.DataFrame(results)
results_df.to_excel("predicted_abstracts_crossencoder.xlsx", index=False)
print("Predictions saved to 'predicted_abstracts_crossencoder.xlsx'")

